{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "import context_changers\n",
    "import ct_model\n",
    "import dmc\n",
    "import drqv2\n",
    "import utils\n",
    "import numpy as np\n",
    "import rl_model\n",
    "\n",
    "import imageio\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "task_name = 'reacher_hard'\n",
    "expert_frame_stack = 3\n",
    "frame_stack = 1\n",
    "action_repeat = 2\n",
    "seed = 432335\n",
    "xml_path = 'domain_xmls/reacher.xml'\n",
    "episode_len = 30\n",
    "context_camera_ids = [0]\n",
    "learner_camera_id = 0\n",
    "im_w = 64\n",
    "im_h = 64\n",
    "n_video = 64\n",
    "cam_id = random.choice(context_camera_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "utils.set_seed_everywhere(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading of the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expert: drqv2.DrQV2Agent = drqv2.DrQV2Agent.load('experts/reacher_hard.pt')\n",
    "expert.train(training=False)\n",
    "\n",
    "context_translator: ct_model.CTNet = ct_model.CTNet.load('ct/reacher_hard.pt').to(utils.device())\n",
    "context_translator.eval()\n",
    "\n",
    "agent: rl_model.ACAgent = rl_model.ACAgent.load('ac/reacher_hard.pt').to(utils.device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading and wrapping of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expert_env = dmc.make(task_name, expert_frame_stack, action_repeat, seed, xml_path, episode_len=episode_len)\n",
    "context_changer = context_changers.ReacherHardContextChanger()\n",
    "\n",
    "eval_env = dmc.make(task_name, frame_stack, action_repeat, seed + 1, xml_path, learner_camera_id, im_w, im_h, context_changers.ReacherHardContextChanger(), episode_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_expert_video():\n",
    "    with torch.no_grad():\n",
    "        videos = []\n",
    "        for _ in range(n_video):\n",
    "            context_changer.reset()\n",
    "\n",
    "            cam_id = random.choice(context_camera_ids)\n",
    "            episode = []\n",
    "            time_step = expert_env.reset()\n",
    "\n",
    "            with utils.change_context(expert_env, context_changer):\n",
    "                episode.append(expert_env.physics.render(im_w, im_h, camera_id=cam_id))\n",
    "            while not time_step.last():\n",
    "                action = expert.act(time_step.observation, 1, eval_mode=True)\n",
    "                time_step = expert_env.step(action)\n",
    "                with utils.change_context(expert_env, context_changer):\n",
    "                    episode.append(expert_env.physics.render(im_w, im_h, camera_id=cam_id))\n",
    "            videos.append(episode)\n",
    "        videos = np.array(videos, dtype=np.uint8)  # n_video x T x h x w x c\n",
    "        videos = videos.transpose((0, 1, 4, 2, 3))  # n_video x T x c x h x w\n",
    "    return videos\n",
    "\n",
    "def predict_avg_states_frames(fobs):\n",
    "    expert_videos = make_expert_video()\n",
    "    with torch.no_grad():\n",
    "        states = []\n",
    "        frames = []\n",
    "\n",
    "        fobs = torch.tensor(fobs, device=utils.device(), dtype=torch.float)\n",
    "        expert_videos = torch.tensor(expert_videos, device=utils.device(), dtype=torch.float)\n",
    "        for expert_video in expert_videos:\n",
    "            state, frame = context_translator.translate(expert_video, fobs, keep_enc2=True)\n",
    "            states.append(state)\n",
    "            frames.append(frame)\n",
    "        states = torch.stack(states)  # n x T x z\n",
    "        frames = torch.stack(frames)  # n x T x c x h x w\n",
    "\n",
    "        avg_states = states.mean(dim=0)  # T x z\n",
    "        avg_frames = frames.mean(dim=0)  # T x c x h x w\n",
    "        print(frames.flatten(start_dim=1).var(dim=0).sum())\n",
    "\n",
    "    avg_states = avg_states.cpu().numpy()\n",
    "    avg_frames = avg_frames.cpu().numpy()\n",
    "\n",
    "    return expert_videos, avg_states, avg_frames\n",
    "\n",
    "\n",
    "def change_step_observation(time_step, target_state, target_frame):\n",
    "    with torch.no_grad():\n",
    "        obs = torch.tensor(time_step.observation, device=utils.device(), dtype=torch.float)\n",
    "        state = context_translator.encode(obs.unsqueeze(0))[0].cpu().numpy()\n",
    "    state = np.concatenate([state, target_state])\n",
    "    return time_step._replace(observation=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building of the agent video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent_video = []\n",
    "\n",
    "time_step = eval_env.reset()\n",
    "frame = time_step.observation\n",
    "expert_videos, avg_states, avg_frames = predict_avg_states_frames(frame)\n",
    "target_state, target_frame = avg_states[1], avg_frames[1]\n",
    "time_step = change_step_observation(time_step, target_state, target_frame)\n",
    "episode_step = 0\n",
    "\n",
    "agent_video.append(frame.transpose((1, 2, 0)))\n",
    "while not time_step.last():\n",
    "    with torch.no_grad(), utils.eval_mode(agent):\n",
    "        state = torch.tensor(time_step.observation, device=utils.device(), dtype=torch.float)\n",
    "        action = agent.act(state, 1, eval_mode=True)\n",
    "\n",
    "    time_step = eval_env.step(action)\n",
    "    episode_step += 1\n",
    "    if episode_step + 1 < avg_states.shape[0]:\n",
    "        target_state = avg_states[episode_step + 1]\n",
    "        target_frame = avg_frames[episode_step + 1]\n",
    "\n",
    "    agent_video.append(time_step.observation.transpose((1, 2, 0)))\n",
    "    time_step = change_step_observation(time_step, target_state, target_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent_video = np.array(agent_video)\n",
    "source_video = expert_videos[1].cpu().numpy().transpose((0, 2, 3, 1))\n",
    "predicted_video = avg_frames.transpose((0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_video = np.zeros( (source_video.shape[0], source_video.shape[1], source_video.shape[2] * 3, source_video.shape[3]))\n",
    "\n",
    "all_video[:, :, 0:64, :] = source_video\n",
    "all_video[:, :, 64:128:, :] = predicted_video\n",
    "all_video[:, :, 128:, :] = agent_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generation of the final video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The video path is `demo/demo_ifo.mp4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imageio.mimwrite('demo/demo_ifo.mp4', all_video, format='mp4', fps=24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}